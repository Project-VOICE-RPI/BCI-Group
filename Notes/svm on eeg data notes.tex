\documentclass{article}
\usepackage{url} % For handling URLs
\usepackage{hyperref} % For clickable links
\usepackage{parskip} % For paragraph spacing

\title{Support Vector Machines (SVMs) and EEG Analysis}
\author{}
\date{}

\begin{document}

\maketitle

\section{Support Vector Machines (SVMs)}
Classifier learning can be seen as finding a curve that best separates two sets of points. The goal is to find an \textbf{optimal curve} out of infinitely many possibilities. However, defining "optimal" is challenging. For example, if we define the optimal curve as the one that minimizes mistakes within the two sets, it may not generalize well to unseen data—a problem known as \textbf{overfitting}.

Support Vector Machines (SVMs) address overfitting by focusing on the points in the dataset that are closest to the line separating the two classes; these points are called \textbf{support vectors}. SVMs maximize the \textbf{margin} (distance) between the support vectors and the separating line. After finding this line using the margins, the SVM creates a formula called the \textbf{weight vector}, which indicates the importance of each feature in deciding the classification.

For classification, the features of an object are plugged into the weight vector to get a score:
\begin{itemize}
    \item A \textbf{positive score} indicates one class.
    \item A \textbf{negative score} indicates the other class.
\end{itemize}

\subsection{Feature Scaling}
Before training, it’s essential to ensure that different features are on a similar scale to prevent bias from features with large numbers. This is typically done by:
\begin{enumerate}
    \item Subtracting the mean of each feature.
    \item Dividing by the standard deviation of each feature.
\end{enumerate}

\section{Principal Component Analysis (PCA)}
Principal Component Analysis (PCA) is a technique used for dimensionality reduction and feature extraction. It transforms data into a lower-dimensional space while preserving the most important patterns or "components." In EEG classification, PCA helps reduce the complexity of high-dimensional data while retaining the most informative features for classification tasks like P300 detection.

\subsection{Steps in PCA}
PCA involves the following steps:
\begin{enumerate}
    \item \textbf{Standardization}: Center the data by subtracting the mean and scaling to unit variance for each feature.
    \item \textbf{Covariance Matrix Computation}: Calculate the covariance matrix to understand how different features vary with respect to each other.
    \item \textbf{Eigenvalue and Eigenvector Calculation}: Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors represent directions (principal components), and the eigenvalues represent the magnitude of variance along those directions.
    \item \textbf{Select Principal Components}: Sort eigenvectors by their eigenvalues in descending order and select the top components that capture the majority of the variance.
    \item \textbf{Transform the Data}: Project the original data onto the selected principal components, resulting in a reduced feature set.
\end{enumerate}

\subsection{Benefits of PCA in EEG Analysis}
Using PCA in EEG analysis, specifically for P300 classification, provides several benefits:
\begin{itemize}
    \item \textbf{Noise Reduction}: By focusing on the components with the most variance, PCA can help eliminate noise and irrelevant features from EEG data.
    \item \textbf{Dimensionality Reduction}: EEG data often has high dimensionality (many channels and time points), which can be computationally expensive. PCA reduces the dimensionality, making it more manageable for machine learning algorithms like SVM.
    \item \textbf{Improved Classifier Performance}: Reducing irrelevant features can improve the accuracy and generalization of classifiers by focusing on the most informative patterns.
\end{itemize}

\section{Application of SVMs in EEG-Based P300 Detection}
In EEG-based Brain-Computer Interfaces (BCIs), detecting the P300 event-related potential is crucial for identifying responses to stimuli. This section summarizes key steps in applying SVMs to classify P300 responses in EEG data.

\subsection{Data Preprocessing}
To extract meaningful features from EEG data, the following steps are typically applied:
\begin{enumerate}
    \item \textbf{Bandpass Filtering}: Apply a bandpass filter to remove noise outside the target frequency range.
    \item \textbf{Epoch Extraction}: Extract time segments (epochs) aligned with stimuli to capture P300 responses.
    \item \textbf{Feature Extraction with PCA}: Use Principal Component Analysis (PCA) to reduce the dimensionality of the data while retaining the most informative components.
\end{enumerate}

\subsection{Training and Classification with SVM}
After preprocessing, the reduced feature set is used to train an SVM classifier to differentiate between target and non-target responses. The classifier’s goal is to maximize the separation (margin) between P300 and non-P300 responses, enhancing single-trial classification accuracy.

\section{References}
\begin{itemize}
    \item Bernard Ng, Rebecca K. Reh, Sara Mostafavi, \emph{A Practical Guide to Applying Machine Learning to Infant EEG Data}, Developmental Cognitive Neuroscience, Volume 54, 2022, 101096, ISSN 1878-9293. \href{https://doi.org/10.1016/j.dcn.2022.101096}{https://doi.org/10.1016/j.dcn.2022.101096}
\end{itemize}

\end{document}
